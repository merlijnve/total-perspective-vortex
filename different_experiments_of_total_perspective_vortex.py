# -*- coding: utf-8 -*-
"""Different experiments of Total Perspective Vortex

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UIqRnmXo3nz7MkSVcZNKvH554FRPHlwL

# Retrieving the dataset from physionet

1. Left or right fist [R3, R7, R11]
2. Imagine left or right fist [R4, R8, R12]
3. Both fists or feet [R5, R9, R13]
4. Imagine both fists or feet [R6, R10, R14]
5. Baseline eyes open [R1]
6. Baseline eyes closed [R2]
"""

# !wget -r -N -c -np https://physionet.org/files/eegmmidb/1.0.0/

"""# V.1.1 Preprocessing, parsing and formatting

First, you’ll need to parse and explore EEG data with MNE, from physionet. You will
have to write a script to visualize raw data and then filter it to keep only useful frequency
bands, and visualize again after this preprocessing.

This part is where you’ll decide which features you’ll extract from the signals to feed them
to your algorithm. So you’ll have to be thorough in picking what matters for the desired
output.

One example is to use the power of the signal by frequency and by channel to the pipeline’s
input.
Most of the algorithms linked to filtering and obtaining the signal’s specter use Fourier
transform or wavelet transform (cf. bonus).

**Experiment structure**

T0 corresponds to rest

T1 corresponds to onset of motion (real or imagined) of:
  - the left fist (in runs 3, 4, 7, 8, 11, and 12)
  - both fists (in runs 5, 6, 9, 10, 13, and 14)

T2 corresponds to onset of motion (real or imagined) of:
  - the right fist (in runs 3, 4, 7, 8, 11, and 12)
  - both feet (in runs 5, 6, 9, 10, 13, and 14)

Therefore there are 5 targets:
  - Rest
  - Left fist
  - Right fist
  - Both fists
  - Both feet

**DATASET STRUCTURE:**

```
dataset/S{subject_nr}/S{subject_nr}{run_nr}.edf
```
"""

# !pip install mne

"""**Set dataset information**"""

experiments = [
    # {
    #     "name": "Left_right_fist",
    #     "description": "open and close left or right fist",
    #     "runs": [3, 7, 11],
    #     "mapping": {0: "Rest", 1: "Left fist", 2: "Right fist"},
    # },
    {
        "name": "Imagine_left_right_fist",
        "description": "imagine opening and closing left or right fist",
        "runs": [4, 8, 12],
        "mapping": {0: "Rest", 1: "Imagine left fist", 2: "Imagine right fist"},
    },
    # {
    #     "name": "Fists_feet",
    #     "description": "open and close both fists or both feet",
    #     "runs": [5, 9, 13],
    #     "mapping": {0: "Rest", 1: "Both fists", 2: "Both feet"},
    # },
    # {
    #     "name": "Imagine_fists_feet",
    #     "description": "imagine opening and closing both fists or both feet",
    #     "runs": [6, 10, 14],
    #     "mapping": {0: "Rest", 1: "Imagine both fists", 2: "Imagine both feet"},
    # },
    # {
    #     "name": "Movement_of_fists",
    #     "description": "movement (real or imagined) of fists",
    #     "runs": [3, 7, 11, 4, 8, 12],
    #     "mapping": {0: "Rest", 1: "Left fist", 2: "Right fist"},
    # },
    # {
    #     "name": "Movement_fists_feet",
    #     "description": "movement (real or imagined) of fists or feet",
    #     "runs": [5, 9, 13, 6, 10, 14],
    #     "mapping": {0: "Rest", 1: "Both fists", 2: "Both feet"},
    # },
]

PATH = "/home/mvan-eng/goinfre/physionet.org/files/eegmmidb/1.0.0/"

"""**Creating a metadata array containing information about all runs**"""

def run_key(subject_nr=1, run_nr=1):
  subject = "S{:03d}".format(subject_nr)
  run = "R{:02d}".format(run_nr)
  return subject + run

def get_filepath(subject_nr=1, run_nr=1):
  subject = "S{:03d}".format(subject_nr)
  run = "R{:02d}".format(run_nr)
  filename = subject + "/" + subject + run + ".edf"
  filepath = PATH + filename
  return filepath

def make_runs():
  runs = []
  for subject_nr in range(1, amount_of_subjects + 1):
    for run_nr in range(3, amount_of_runs + 1):
      filepath = get_filepath(subject_nr=subject_nr, run_nr=run_nr)
      runs.append([run_nr, filepath])
  return runs

"""**Read dataset**"""

import mne
import numpy as np

def get_mapping(run_nr):
  event_mapping = {}
  for e in experiments:
    if run_nr in e["runs"]:
      event_mapping = e["mapping"]
  return event_mapping

def read_dataset(ex_nr, batch, start):
  raws = []

  batch_counter = batch
  start_counter = start
  for r in runs:
    if r[0] in experiments[ex_nr]["runs"]:
      if batch_counter == 0:
        break
      if start_counter == 0:
        # Read from new .edf file
        raw = mne.io.read_raw_edf(r[1], preload=True)
        # Resample to match frequencies
        if raw.info['sfreq'] != 160.0:
            raw.resample(sfreq=160.0)
        mne.datasets.eegbci.standardize(raw)
        raw.set_montage("standard_1005")

        events, _ = mne.events_from_annotations(raw, event_id=dict(T1=1, T2=2))
        mapping = get_mapping(r[0])
        annot_from_events = mne.annotations_from_events(events=events,
                                                    event_desc=mapping,
                                                    sfreq=raw.info["sfreq"])
        raw.set_annotations(annot_from_events)
        raws.append(raw)
        batch_counter -= 1
      else:
         start_counter -= 1
  
  if len(raws) == 0:
    return None
  raw = mne.concatenate_raws(raws)
  raw = filter_raw(raw)

  channels = raw.info["ch_names"]
  good_channels = ["FC5", "FC3", "FC1", "FCz", "FC2", "FC4", "FC6",
                            "C5",  "C3",  "C1",  "Cz",  "C2",  "C4",  "C6",
                          "CP5", "CP3", "CP1", "CPz", "CP2", "CP4", "CP6"]

  bad_channels = [x for x in channels if x not in good_channels]
  raw.drop_channels(bad_channels)
  return raw

"""**Plot an instance of raw data**"""

# raw = experiments[0]["raw"]
# def plot_raw():
#   if plotting:
#     raw.plot(scalings=dict(eeg=250e-6), events=events)

# events, event_dict = mne.events_from_annotations(raw)
# plot_raw()

"""**Plot the (average) Power Spectral Density of a raw instance**

The power spectral density (PSD) of the signal describes the power present in the signal as a function of frequency, per unit frequency.
"""

import numpy as np

# def plot_psd_raw(raw=raw, fmin=0, fmax=np.inf):
#   if plotting:
#     psd = raw.compute_psd(fmin=fmin, fmax=fmax)
#     psd.plot(average=True)

# plot_psd_raw()

"""**Filtering**

- Simple bandpass
- Notch filter to filter out 60hz electrical signals
"""

f_low = 7.0
f_high = 35.0

def filter_raw(raw):
  raw.notch_filter(60, method="iir")
  raw.filter(f_low, f_high, fir_design="firwin", skip_by_annotation="edge")
  return raw

# plot_psd_raw(experiments[0]["raw_filtered"])

"""**Using ICA to take out EOG artifacts**"""

from mne.preprocessing import ICA

def apply_ica(ex):
    ica = ICA(n_components=20, random_state=42)
    ica.fit(ex["raw"])
    for channel in ex["raw"].info["ch_names"]:
        bad_idx, scores = ica.find_bads_eog(ex["raw"], ch_name=channel, threshold=2)
    ica.exclude = bad_idx
    ica.apply(ex["raw"])
    return len(bad_idx)

"""**Creating epochs**

In the MNE-Python library, an "epoch" is a defined time window of EEG (Electroencephalography) or MEG (Magnetoencephalography) data that is extracted from continuous data based on specific events or triggers.
"""

from mne import pick_types

def create_epochs(raw):
    tmin = -1.000  # start of each epoch (in sec)
    tmax = 2.000  # end of each epoch (in sec)
    baseline = (None, 0)
    picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude="bads")

    epochs = mne.Epochs(raw,
                         events=events,
                         event_id=event_dict,
                         tmin=tmin, tmax=tmax,
                         baseline=baseline,
                         picks=picks,
                         proj=True,
                         preload=True)

    return epochs


# """**Plotting an evoked**"""

# def plot_evoked(evoked):
#   if plotting:
#     evoked.plot(gfp=True)
#     evoked.plot_topomap(times=[-0.2, 0.2, 0.4, 0.6, 0.8], average=0.05)
#     evoked

# if plotting:
#   plot_evoked(experiments[0]["epochs"]['Left fist'].average())

# if plotting:
#   experiments[0]["epochs"]['Left fist'].plot_image(picks=["Cz"])

# # **Creating data and targets**

# # X = Power Spectral Density of epochs
# # def psd_from_epochs(epochs):
# #   psd_shape = epochs[0].compute_psd(fmin=f_low, fmax=f_high).get_data().shape
# #   X = np.empty((len(epochs), psd_shape[1], psd_shape[2]))
# #   for i in range(len(epochs)):
# #     X[i] = epochs[i].compute_psd(fmin=f_low, fmax=f_high).get_data()
# #   return X

# # for ex in experiments:
# #    ex["X"] = psd_from_epochs(ex["epochs"])

# # y = epochs.events[:, -1]
# # print(y.shape)

"""# V.1.2 Treatment pipeline

Then the processing pipeline has to be set up:

• Dimensionality reduction algorithm (i.e.: PCA, ICA, CSP, CSSP...)

• Classification algorithm, there is plenty of choice among those available in sklearn,
to output the decision of what data chunk corresponds to what kind of motion.

• "Playback" reading on the file to simulate a data stream.

It is advised to first test your program architecture with sklearn and MNE algorithms, before implementing your own CSP or whatever algorithm you chose.

The program will have to contain a script for training and a script for prediction.
The script predicting output will have to do it on a stream of data, and within a delay
of 2s after the data chunk was sent to the processing pipeline. (You should not use
mne-realtime)

You have to use the pipeline object from sklearn (use baseEstimator and transformerMixin classes of sklearn)

"""
"""**Visualize evokeds**"""

# epochs = experiments[0]["epochs"]
# if plotting:
#   mne.viz.plot_compare_evokeds(
#       dict(left_fist=epochs["Left fist"].average(),
#           right_fist=epochs["Right fist"].average()),
#       legend="upper left"
#   )

# if plotting:
#   epochs["Left fist"].average().plot_joint(picks="eeg")
#   _ = plot_evoked(epochs["Left fist"].average())

# if plotting:
#   epochs["Left fist"].average().compute_psd().plot()
#   epochs["Right fist"].average().compute_psd().plot()

from mne.decoding import CSP
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import ShuffleSplit, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import LinearSVC

cv = ShuffleSplit(10, test_size=0.2, random_state=42)

def make_clf():
  csp = CSP(n_components=6)
  lda = LinearDiscriminantAnalysis(solver="lsqr")

  clf = Pipeline([
      ("CSP", csp),
      ('Polynomial', PolynomialFeatures()),
      ("LDA", lda)
      ])
  return clf

# """# V.1.4 Train, Validation, and Test

# • You have to use cross_val_score on the whole processing pipeline, to evaluate your
# classification.

# • You must choose how to split your data set between Train, Validation, and Test set
# (Do not overfit, with different splits each time)

# • You must have 60% mean accuracy on all subjects used in your Test Data (corresponding to the six types of experiment runs and on never-learned data)

# • You can train/predict on the subject and the task of your choice
# """

import math

def split_train_test(experiment):
  X = experiment["X"]
  y = experiment["y"]
  test_amount = math.ceil(0.20 * len(X))

  X_test = X[:test_amount]
  y_test = y[:test_amount]

  X_train = X[test_amount:]
  y_train = y[test_amount:]

  return X_test, y_test, X_train, y_train


# from sklearn.model_selection import LearningCurveDisplay
# import matplotlib.pyplot as plt

# # def learning_curve_from_experiment(experiment):
# #   X = experiment["X"]
# #   y = experiment["y"]
# #   clf = experiment["clf"]
# #   fig, ax = plt.subplots(figsize=(10, 6), sharey=True)
# #   common_params = {
# #       "X": X,
# #       "y": y,
# #       "train_sizes": np.linspace(0.1, 1.0, 10),
# #       "cv": cv,
# #       "score_type": "both",
# #       "n_jobs": 4,
# #       "line_kw": {"marker": "o"},
# #       "std_display_style": "fill_between",
# #       "score_name": "Accuracy",
# #   }

# #   LearningCurveDisplay.from_estimator(clf, **common_params, ax=ax, verbose=1)
# #   handles, label = ax.get_legend_handles_labels()
# #   ax.legend(handles[:2], ["Training Score", "Test Score"])
# #   ax.set_title(f"Learning Curve for {clf.__class__.__name__}")
# #   plt.show()

# # learning_curve_from_experiment(experiments[0])

import joblib
from sklearn.metrics import accuracy_score

def dump_model(ex):
          joblib.dump(ex["clf"], f'{ex["name"]}_{amount_of_subjects}_subjects.save')

amount_of_subjects = 109
amount_of_runs = 14
batch_read = 50

plotting = False
ica = False

test_scores = []
train_scores = []
crossval_scores = []
accuracy_scores = []
for i, ex in enumerate(experiments):
  runs = make_runs()
  ex["epochs"] = []

  batch_start = 0
  buffer = read_dataset(i, batch_read, batch_start)
  while buffer != None:
    ex["raw"] = buffer
    events, event_dict = mne.events_from_annotations(ex["raw"])
    batch_start += batch_read

    if ica:
      rank = apply_ica(ex)

    ex["epochs"].append(create_epochs(ex["raw"]))
    del ex["raw"]
    buffer = read_dataset(i, batch_read, batch_start)

  ex["epochs"] = mne.concatenate_epochs(ex["epochs"])
  ex["X"] = ex["epochs"].get_data()
  ex['y'] = ex["epochs"].events[:, -1]
  
  X_test, y_test, X_train, y_train = split_train_test(ex)
  ex["clf"] = make_clf()
  ex["clf"].fit(X_train, y_train)
  y_test_pred = ex["clf"].predict(X_test)

  ex["scores"] = cross_val_score(ex["clf"], X_train, y_train, cv=cv, error_score='raise')
  train_score = ex["clf"].score(X_train, y_train)
  test_score = ex["clf"].score(X_test, y_test)
  accuracy = accuracy_score(y_test, y_test_pred)

  print(ex["name"])
  print("Train: ", train_score)
  train_scores.append(train_score)
  
  print("Test: ", test_score)
  test_scores.append(test_score)
  
  print("Crossval: %f" % (np.mean(ex["scores"])))
  crossval_scores.append(np.mean(ex["scores"]))

  print("Accuracy: ", accuracy)
  accuracy_scores.append(accuracy)
  print()

  dump_model(ex)
  del ex

print("Mean scores")
print("Train: ", round(sum(train_scores) / len(train_scores), 2))
print("Test: ", round(sum(test_scores) / len(test_scores), 2))
print("Crossval: ", round(sum(crossval_scores) / len(crossval_scores), 2))
print("Accuracy: ", round(sum(accuracy_scores) / len(accuracy_scores), 2))
